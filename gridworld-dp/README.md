
# ğŸŒ Gridworld DP

This project explores **Dynamic Programming (DP)** techniques in the classic **Gridworld** environment, inspired by **Chapter 4** of _Reinforcement Learning: An Introduction_ by **Sutton & Barto**.

---

## ğŸ“– References

This implementation is based on the concepts from:

- **_Reinforcement Learning: An Introduction_**  
  **Richard S. Sutton & Andrew G. Barto**  
  _Second Edition, MIT Press, 2018_  
  [View Book (PDF)](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

---

## ğŸ“ Project Structure

```
gridworld-dp/
â”‚
â”œâ”€â”€ src/                          # Core logic and environment
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ grid_world.py             # DP algorithms and Gridworld logic
â”‚
â”œâ”€â”€ notebooks/                    # Interactive notebooks for experiments
â”‚   â””â”€â”€ grid_world.ipynb
â”‚
â”œâ”€â”€ book_images/                  # Original images from the textbook
â”‚   â”œâ”€â”€ Example_4_1.PNG
â”‚   â””â”€â”€ Figure_4_1.PNG
â”‚
â”œâ”€â”€ generated_images/             # Output visuals from simulations
â”‚   â”œâ”€â”€ figure_4_1_in_place.png
â”‚   â””â”€â”€ figure_4_1_out_place.png
â”‚
â””â”€â”€ README.md                     # Project overview and documentation
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/AraratDavtyan/gridworld-dp.git
cd gridworld-dp
```

### 2ï¸âƒ£ Install the dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Launch the notebook

```bash
jupyter notebook notebooks/grid_world.ipynb
```

---

## ğŸ§  Key Highlights

- âœ… In-place and out-of-place **policy evaluation** strategies
- âœ… Clear implementation of **policy iteration**
- âœ… Visualization of the **value function** updates
- âœ… Organized structure for experiments and codebase
- âœ… Great for understanding **model-based RL** foundations

---

## ğŸ“Š Visual Results

### ğŸ” In-Place Policy Evaluation

Each state's value is updated immediately using the most recent estimates.

ğŸ“ˆ **Result:**

![In-Place](generated_images/figure_4_1_in_place.png)

---

### ğŸ“¤ Out-of-Place Policy Evaluation

The value function is updated all at once using a copy of the previous iteration.

ğŸ“ˆ **Result:**

![Out-of-Place](generated_images/figure_4_1_out_place.png)

---

## ğŸ¯ Conclusion

This project helps solidify foundational concepts in **Dynamic Programming for RL**, including:

- Understanding how agents plan using models of the environment
- Differentiating between **asynchronous (in-place)** and **synchronous (out-of-place)** updates
- A strong stepping stone toward more advanced RL methods like **Monte Carlo** and **Temporal-Difference Learning**

---
